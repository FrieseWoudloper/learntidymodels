---
title: "Unsupervised dimensionality reduction with tidymodels"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    css: css/style.css
runtime: shiny_prerendered
---
<script language="JavaScript" src="js/exercise-font-size.js"></script>

```{r setup, include=FALSE, message=FALSE}
library(learnr)
library(knitr)
library(tidyverse)
library(recipes)
library(embed)
library(corrr)
library(tidytext)
library(gradethis)
library(sortable)

knitr::opts_chunk$set(echo = FALSE, exercise.checker = gradethis::grade_learnr)

zoo_names <- c("animal_name", "hair", "feathers", "eggs", "milk", "airborne", "aquatic", "predator", "toothed", "backbone", "breathes", "venomous", "fins", "legs", "tail", "domestic", "catsize", "class")
anim_types <- tribble(~class, ~type,
                      1, "mammal",
                      2, "bird",
                      3, "reptile",
                      4, "fish",
                      5, "amphibian",
                      6, "insect",
                      7, "other_arthropods")
zoo <- 
  read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data", 
           col_names = zoo_names) %>%
  left_join(anim_types) %>%
  select(-class) %>%
  rename(animal_type=type)

zoo_corr <- zoo %>%
  select(-animal_name, -animal_type) %>%
  correlate() %>%
  rearrange()

pca_rec <- recipe(data = zoo, formula = ~.) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)
pca_loading <- tidy(pca_prep, 3)
pca_bake <- bake(pca_prep, zoo)
sdev <- pca_prep$steps[[3]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

exp_var <- tibble(
  component = unique(pca_loading$component),
  percent_var = percent_variation) %>%
  mutate(component = fct_inorder(component))
```



## Welcome

Dimension reduction is a regularly used unsupervised method in exploratory data analysis and predictive models.

This tutorial will teach you how to apply these methods using the [recipes](https://recipes.tidymodels.org/) package, which is a part of the [tidymodels](https://www.tidymodels.org) ecosystem, a collection of modeling packages designed with common APIs and a shared philosophy.

<!-- The [recipes](https://recipes.tidymodels.org/) package is designed to help you preprocess your data _before_ training or fitting a model and contains functions for a wide range of preprocessing steps, such as: -->

<!-- + converting qualitative predictors to indicator variables (also known as dummy variables),   -->
<!-- + transforming data to be on a different scale (e.g., taking the logarithm of a variable),   -->
<!-- + transforming whole groups of predictors together,    -->
<!-- + extracting key features from raw variables (e.g., getting the day of the week out of a date variable), -->

<!-- and many more. While this might sound similar to R's _formula_ interface, **recipes** provide a plethora of additional tools for preprocessing steps, which can be searched and further explored [here](https://www.tidymodels.org/find/recipes/). -->

### Learning objectives

This tutorial focuses on _transforming whole groups of predictors together_ using two different dimension reduction algorithms:

1. Linear dimensionality reduction with [Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)   
2. Non-linear dimensionality reduction with [UMAP](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html)

Here, we will apply these methods to explore our data. These methods can also be used for [feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) prior to modeling. 

While we're applying these methods we will cover:

+ How to create a `recipe`
+ How to update a `role`
+ How to add `steps`
+ How to `prep`
+ How to `bake` or `juice`

### Pre-requisites 

If you are new to [tidymodels](https://www.tidymodels.org), you can learn what you need with the five [Get Started articles on tidymodels.org](https://www.tidymodels.org/start/). 

The second article, [Preprocessing your data with recipes](https://www.tidymodels.org/start/recipes/), shows how to use functions from the [recipes](https://recipes.tidymodels.org/) package to pre-process your data prior to model fitting. 

If you aren't familiar with the **recipes** functions yet, reading the [Preprocessing your data with recipes](https://www.tidymodels.org/start/recipes/) article would be helpful before going through this tutorial.  

Let's get started!

## The zoo data

We will use the `zoo` dataset to explore these methods. `zoo` contains observations collected on `r nrow(zoo)` zoo animals.

To see the first ten rows of the data set click on **Run Code**.   
You can use the black triangle that appears at the top right of the table to scroll through all of the columns in `zoo`.


```{r intro-zoo, exercise=TRUE}
zoo
```


Alternatively, use `glimpse()` from the [tidyverse](https://www.tidyverse.org/) metapackage to see columns in a more compact way. You can click on the **Solution** button to get help.

```{r glimpse-zoo, exercise=TRUE, warning=FALSE, message=FALSE, error=FALSE}
library(tidyverse)
glimpse(___)
```

```{r glimpse-zoo-solution}
glimpse(zoo)
```

We can see that `zoo` has `r nrow(zoo)` rows and `r ncol(zoo)` columns, two of them (`animal_name` and `animal_type`) are characters. 

Let's count the number animals for each `animal_type`.

```{r cnt-type, exercise=TRUE}
zoo %>% 
  count(___)
```

```{r cnt-type-solution}
zoo %>% 
  count(animal_type)
```

While looking at the numbers helps, plotting is always a good idea to get an overall view of the data, especially if many sub-categories are present.

Plot the number of animals in each `animal_type` category. Fill in the blanks and click on **Run Code** to generate the plot.

```{r type-bar, exercise=TRUE}
zoo %>%
  ggplot(aes(___)) +
  geom____(fill="#CA225E") +
  theme_minimal()
```

```{r type-bar-solution}
zoo %>%
  ggplot(aes(animal_type)) +
  geom_bar(fill="#CA225E") +
  theme_minimal()
```

We can also look at the distribution of animals that lay `eggs` across animal types. The `eggs` column is coded in `0` and `1` for animals that "doesn't lay eggs" and "lay eggs" respectively. Let's do some data wrangling with `recode` function to plot it neatly. Click on **Solution** if you are stuck.

```{r type-eggs-bar, exercise=TRUE}
zoo %>%
  mutate(eggs = recode(eggs, 0=___, 1=___)) %>%
  ggplot(aes(___, fill=___)) +
  geom___() +
  scale_fill_manual(values = c("#372F60", "#CA225E")) +
  theme_minimal() +
  theme(legend.position = "top")
```

```{r type-eggs-bar-solution}
zoo %>%
  mutate(eggs = recode(eggs, `0`="doesn't lay eggs", `1`="lays Eggs" )) %>%
  ggplot(aes(animal_type, fill=eggs)) +
  geom_bar() +
  scale_fill_manual(values = c("#372F60", "#CA225E")) + 
  theme_minimal() +
  theme(legend.position = "top")
```

Not so surprisingly there are very few mammals that lay eggs. 
Let's get the actual count.

```{r cnt-type-eggs, exercise=TRUE}
zoo %>% 
  count(___, ___)
```

```{r cnt-type-eggs-solution}
zoo %>% 
  count(animal_type, eggs)
```

It looks like there is one mammal that lays eggs!
Can you find the name of that animal?

```{r find-eggs, exercise=TRUE}
zoo %>%
  filter(___ == ___) %>%
  # select relevant columns for a compact view
  select(animal_name, animal_type, eggs) 
```

```{r find-eggs-solution, eval=FALSE}
zoo %>%
  filter(animal_type == "mammal",
         eggs == 1) %>%
  # select relevant columns for a compact view
  select(animal_name, animal_type, eggs) 
```

## Correlation matrix

Having some familiarity with the animal kingdom, we would expect that most animals who produce milk do not lay eggs. In other words, we would expect to see a negative correlation between these features.

Let's see how these animal features correlate with each other to get a sense of these relationships.

Run the code to plot the correlation matrix using the [corrr](https://cran.r-project.org/web/packages/corrr/vignettes/using-corrr.html) package. The `correlate()` function generates a correlation matrix in **data frame** format, and `rearrange()` groups highly correlated variables closer together.

```{r corr, exercise=TRUE, message=FALSE, warning=FALSE, error=FALSE}
library(corrr)
zoo_corr <- zoo %>%
  # drop non-numeric columns
  select(-___, -___) %>%
  correlate() %>%
  rearrange()
zoo_corr
```

```{r corr-solution}
library(corrr)
zoo_corr <- zoo %>%
  # drop non-numeric columns
  select(-animal_name, -animal_type) %>%
  correlate() %>%
  rearrange()
zoo_corr
```

The output is a data frame containing pair-wise correlation coefficients between variables. But it would take us a long time to get an overall sense of these relationships just by looking at raw numbers. Let's plot the correlation matrix to help our brains out. `r emo::ji("brain")`

```{r corr_plot, out.width='100%',exercise=TRUE, message=FALSE, warning=FALSE, error=FALSE}
zoo_corr %>%
  rplot(shape = 15, colours = c("#372F60", "white", "#CA225E"), print_cor=TRUE) +
  theme(axis.text.x = element_text(angle = 20, hjust = .6))
```


So much better! 

We can see that producing eggs or milk have a very strong negative correlation. (The only reason it isn't equal to `-1` is the odd ball platypus.)

Now, see if you can answer the question correctly. 

```{r corr-quiz, echo=FALSE}
question("What is the pair of animal features that has the strongest _positive_ correlation?",
         answer("Tail & Backbone"),
         answer("Fins & Aquatic"),
         answer("Milk & Hair", correct = TRUE),
         answer("Feathers & Airborne"),
         incorrect = "Incorrect. While these two features have a positive correlation it is not the strongest.",
         allow_retry = TRUE
         )

```


## Principal component analysis


[Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is a handy data reduction technique that uses covariance or a correlation matrix of a set of observed variables (just like the one we visualized) and summarizes it with a smaller set of linear combinations called principal components (PC). These components are statistically independent from one another and capture the maximum amount of information (i.e. variance) in the original variables. This means these components can be used to combat large inter-variable correlations in a data set statistical modeling. PCA can also help us explore the similarities between observations and groups they belong to.

Here's the scatter plot with the first two principle components (PC1 and PC2) of the zoo data:

```{r}
pca_bake %>%
  ggplot(aes(PC1, PC2, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = "Animal Type") +
  theme_minimal()
```

Each dot on the plot represents an observation (animal) that is colored by the `animal_type` and labeled by `animal_name`. 

Overall, we can see that same type of animals are clustered closely compared to the rest. This suggests that these features (having hair, feathers, or laying eggs etc.) are doing a relatively good job at identifying the clusters within the zoo data.

### Create a recipe

Let's implement principal component analysis (PCA) using [recipes](https://recipes.tidymodels.org/).

First, we initiate our **recipe** with the `zoo` data.

```{r recstart, exercise=TRUE}
pca_rec <- recipe(~., data = ___) 
```

```{r recstart-solution}
pca_rec <- recipe(~., data = zoo) 
```  

Here, we define two arguments:

+ A **formula** with `~.` tells our recipe that we did not define an outcome variable and would like to use all variables for the next steps of the analysis.

+ Our data `zoo`. We are using our entire data set here, but typically this would be a _training set_ for predictive modeling.

Once we initiate the recipe, we can keep adding new [roles](https://tidymodels.github.io/recipes/reference/roles.html) and **steps**. 

For example, we already told recipe to include all variables with our formula; however, we want to exclude identifier column `animal_name` and `animal_type` from our analysis. On the other hand we need these variables later when we are plotting our results. By using `update_role()` we exclude these variables from our analysis **without** completely dropping them in the next steps:

```{r role, exercise=TRUE}
pca_rec <- recipe(~., data = zoo, ) %>%
  # update the role for animal_name and animal_type
  update_role(___, ___, new_role = "id")
```

```{r role-solution}
pca_rec <- recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id")
```

```{r role-check}
pca_rec <- recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id")
```

Try using `summary` to see the defined roles in `pca_rec` and arrange them by `role` column.

```{r role-sum, exercise=TRUE}
summary(___) %>%
  arrange(___)
```

```{r role-sum-solution}
summary(pca_rec) %>%
  arrange(role)
```

We can see that the role of `animal_name` and `animal_type` is now defined as `id` and the remaining variables are listed as `predictor`. 

Good job! Now, let's add some steps to our recipe.

### Add steps to a recipe

Since PCA is a variance maximizing exercise, it is important to scale variables so their variance is commensurable. We will achieve this by adding two _steps_ to our recipe:   

+ `step_scale()` normalizes numeric data to have a standard deviation of one 
+ `step_center()` normalizes numeric data to have a mean of zero.

We can also use the helper function `all_predictors()` to select all the variables that has a role defined as a `predictor`.

```{r step-norm, exercise=TRUE, exercise.lines=6}
pca_rec <-recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  # add steps to scale and center
  step____(all_predictors()) %>%
  step____(all_predictors())
```

```{r step-norm-solution}
pca_rec <-recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())
```

We are ready to add our final step to compute our principle components! Use `step_pca()` to tell the recipe to o convert all variables (except `animal_name` and `animal_type`) into principal components, then print the recipe.

```{r pca-step, exercise=TRUE}
pca_rec <-recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  # add step for PCA computation
  step____(___)

pca_rec
```

```{r pca-step-solution}
pca_rec <-recipe(~., data = zoo, ) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_pca(all_predictors())
pca_rec
```

We can see that `pca_rec` has our _id_ and _predictor_ variables as inputs and the following operations:

+ Scaling for all_predictors
+ Centering for all_predictors
+ No PCA components were extracted.

Are you surprised that we haven't extracted the PCA components yet? This is because so far we only _defined_ our recipe, but did not _train_ it. To get the results from our PCA, we need evaluate our recipe using `prep()`. 

### Prep a recipe

Let's prep our recipe and print the output:
  
```{r pca-prep, exercise=TRUE}
pca_prep <- prep(___)
pca_prep
```

```{r pca-prep-solution}
pca_prep <- prep(pca_rec)
pca_prep
```

Can you see the difference between the outputs of `pca_rec` and `pca_prep`? After _prepping_ we can see that scaling and  centering for, and PCA extraction with all columns of interest has been _trained_.

Let's take a look at the steps this recipe contains with `tidy()`:

```{r tidy-prep, exercise=TRUE}
tidy(___)
```

```{r tidy-prep-solution}
tidy(pca_prep)
```

We can see that three steps are contained in this prepped recipe: (1) scale, (2) center, and (3) pca. With `tidy()` we can extract the _intermediate values_ computed in each step by providing its `number` as an argument to `tidy()`.

For example, you can extract the mean values for each _predictor_ variable from the second step of our recipe (`center`) using the `tidy` method:

```{r tidy-cen, exercise=TRUE}
tidy(___, ___)
```

```{r tidy-cen-solution}
tidy(pca_prep, 2)
```

Using the same method, we can also extract the variable _loadings_ for each component from our `step_pca`:

```{r tidy-pca, exercise=TRUE}
pca_loading <- tidy(___, ___)
```

```{r tidy-pca-solution}
pca_loading <- tidy(pca_prep, 3)
```

##### You can see that these _intermediate values_ can be different for each step, but are always called `values` when extracted with the `tidy` method. You can find the definition of these _internmediate values_ under the *Value* section in the help page of the related step function. For example, type `?step_scale` on your console and scroll down to see the *Value*.

In the PCA setting, `loadings` indicate the correlation between the principal component and the variable. In other words, large loadings suggest that a variable has a strong effect on that principal component.

Let's take a look at loadings we generated with the zoo data for the first four principle components and top six variables with large loadings by plotting them! We will plot the absolute values of the loadings to easily compare them, and color our plot by the direction of the correlation (positive or negative). We will also use the [tidytext](https://www.rdocumentation.org/packages/tidytext/versions/0.2.4) package to neatly order our variables by the effect they have on our four principle components.

```{r pc-cont, exercise=TRUE}
library(tidytext)

pca_loading %>%
  filter(component %in% c("PC1", "PC2", "PC3", "PC4")) %>%
  group_by(component) %>%
  top_n(6, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  scale_fill_manual(values = c("#372F60", "#CA225E")) +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) +
  theme_minimal()

```

It looks like PC1 (first principal component) is mostly about producing milk or eggs, and having hair. Notice the loading direction for milk and hair are the same, which is the opposite of eggs. Do you remember the strongest positive correlation we found? On the other hand, PC2 seems to be about the animal having a fin or being aquatic, both of which have the opposite direction to breathing. Both tails and feathers have a strong correlation with PC3, and finally, PC4 is mostly about being domestic or a predator, which have opposite directions. Overall, we can say that PC1 is mostly about being a _mammal_, PC2 is being a _fish_ or an _aquatic animal_, PC3 is being a _bird_, and PC4 is about being domesticated.

### Bake a recipe

So far we:

1. Defined preprocessing operations with `recipe`   
1. Trained our recipe with `prep` 

Finally, in order to apply these computations to our data and extract the principal components, we will use `bake` by providing two arguments:

1. A prepped (trained) recipe  
1. The data we would like to apply these computations to

```{r bake, exercise=TRUE}
pca_bake <- bake(___, ___)
pca_bake
```

```{r bake-solution}
pca_bake <- bake(pca_prep, zoo)
pca_bake
```

Now that we got our principal components, we are ready to plot them!`r emo::ji("tada")`

Let's plot the first two principal components, while labeling our points with `animal_name` and coloring them by `animal_type`.

```{r pca-plot, exercise=TRUE}
pca_bake %>%
  ggplot(aes(___, ___, label=___)) +
  geom_point(aes(color = ___), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

```{r pca-plot-solution}
pca_bake %>%
  ggplot(aes(PC1, PC2, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

We were able reproduce our initial plot! Let's take a look at our plot in more detail.

Look at _mammals_, majority of these animals are separated from the other types of animals across the PC1 axis. Recall our plot with loadings: `milk` and `eggs` were the top two features with largest loadings for PC1. Interestingly, platypus (our favorite odd ball) is placed closer to _reptiles_ and the penguin on the PC1 axis. This is likely driven by laying eggs and not having teeth. On the other hand, seal and especially dolphin are located closer to _fish_ and the sea snake, and separate from rest of the mammals on the PC2 axis. Do you remember the top two loadings for PC2? It was `fins` and `aquatic`! Do you see the pattern here? 

A common practice when conducting PCA is to check how much variability in the data is captured by principal components. Typically, this is achieved by looking at the eigenvalues or their percent proportion for each component. Let's extract variance explained by each principal component.

```{r var-exp, exercise=TRUE}
sdev <- pca_prep$steps[[3]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

exp_var <- tibble(
  component = unique(pca_loading$component),
  percent_var = percent_variation) %>%
  mutate(component = fct_inorder(component))

exp_var
```

Now let's plot them to help our brains out once more:

```{r var-plot, exercise=TRUE}
exp_var %>%
  ggplot(aes(___, ___)) +
  geom_col(fill="#372F60") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = "Percent variance explained by each PCA component") +
  theme_minimal()
```

```{r var-plot-solution}
exp_var %>%
  ggplot(aes(component, percent_var)) +
  geom_col(fill="#372F60") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = "Percent variance explained by each PCA component") +
  theme_minimal()
```

We can see that first three principal components explain majority of the variance in the data. But it difficult to see the cumulative variance explained in this plot. Let's create a different column plot to see the cumulative variance explained:

```{r cum-var-plot, exercise=TRUE}
exp_var %>%
  mutate(cum_var = cumsum(___)) %>%
  ggplot(aes(___, ___)) +
  geom_col(fill="#372F60") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = "Cumulative percent of variance explained by each PCA component") +
  theme_minimal()
```

```{r cum-var-plot-solution}
exp_var %>%
  mutate(cum_var = cumsum(percent_variation)) %>%
  ggplot(aes(component, cum_var)) +
  geom_col(fill="#372F60") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = NULL, y = "Cumulative percent of variance explained by each PCA component") +
  theme_minimal()
```

We can see that 50% of the variance is explained by the first two components. If we were to use more components, we can capture even more variance in the data. That is why it is also common to plot multiple components to get a better idea of the data.

Try plotting PC1 and PC3, do you see other clusters in the data that wasn't as obvious with PC1 and PC2?

```{r pca3-plot, exercise=TRUE}
pca_bake %>%
  ggplot(aes(___, ___, label=___)) +
  geom_point(aes(color = ___), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

```{r pca3-plot-solution}
pca_bake %>%
  ggplot(aes(PC1, PC3, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

## UMAP

[Uniform manifold approximation and projection (UMAP)](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) is a _non-linear_ graph based dimension reduction algorithm. It finds local, low dimensional representations of the data and can be run unsupervised or supervised with different types of outcome data (e.g. numeric, factor, etc). 

Some of the advantages of using UMAP are:

+ Can capture non-linear relationships in the data     
+ Can handle large, high-dimensional data sets   
+ Has a much shorter computation time than other non-linear graph based dimension reduction algorithms (e.g. t-SNE)   
+ Is able to represent not only within cluster similarities but also global relationships  
+ Can be used as a general-purpose dimensionality reduction technique for data preprocessing prior to modeling 

Now that we learned how to create a recipe for PCA, we can apply our knowledge to create one for UMAP with `zoo` data too! 

```{r umap-sort}
umap_ord <- c(
  "recipe(~., data = zoo) %>%",
  "update_role(animal_name, animal_type, new_role = \"id\") %>%",
  "step_scale(all_predictors()) %>%",
  "step_center(all_predictors()) %>%",
  "step_umap(all_predictors())"
)

question_rank(
  "Sort the following recipe steps to create a recipe with UMAP:",
  answer(umap_ord, correct = TRUE),
  answer(umap_ord[c(1,2,4,3,5)], correct = TRUE),
  allow_retry = TRUE
)

```

Fill in the blanks to create a recipe with UMAP, then prep and bake the recipe to compute UMAP components:

```{r umap-try, exercise=TRUE, exercise.lines=16}
set.seed(123) # set a seed to reproduce random number generation
library(embed) # load the library to use `step_umap()`  

## Create the recipe accordingly
umap_rec <- recipe(~., data = zoo) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step___(all_predictors())

## Train your recipe with prep
umap_prep <- prep(___)

## Apply computations with bake
umap_bake <- bake(___, ___)
umap_bake
```

```{r umap-try-solution}
set.seed(123) # set a seed to reproduce random number generation
library(embed) # load the library to use `step_umap()`  

## Create the recipe accordingly
umap_rec <- recipe(~., data = zoo) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_umap(all_predictors())

## Prep your recipe
umap_prep <- prep(umap_rec)

## Extract UMAP components with bake
umap_bake <- bake(umap_prep, zoo)
umap_bake
```

Great job!

It's time to plot our UMAP components! `r emo::ji("tada")`

```{r umap-plot, exercise=TRUE}
umap_bake %>%
  ggplot(aes(___, ___, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

```{r umap-plot-solution}
umap_bake %>%
  ggplot(aes(umap_1, umap_2, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

Do you think UMAP is doing a better job than PCA?

### UMAP hyperparameters

UMAP algorithm has multiple hyperparameters that can have significant impact on the results. The four major hyperparameters are:

+ `neighbors`   
+ `num_comp`  
+ `min_dist`  
+ `learn_rate`

These hyperparameters can be specified in the `step_umap` function. You can find more details in [`step_umap` help document](https://embed.tidymodels.org/reference/step_umap.html) and further explanations on hyperparameters [here](https://umap-learn.readthedocs.io/en/latest/parameters.html)

Try setting `num_comp` (number of components) to `3` and plot UMAP components again, but this time use the first and the third components:

```{r umap-par, exercise=TRUE, exercise.lines=25}
set.seed(123) # set a seed to reproduce random number generation
library(embed) # load the library to use `step_umap()`  

## Create the recipe accordingly
umap_rec <- recipe(~., data = zoo) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_umap(all_predictors(), ___ = ___)

## Prep your recipe
umap_prep <- prep(umap_rec)

## Extract UMAP components with bake
umap_bake <- bake(umap_prep, zoo)
umap_bake

## Plot UMAP components
umap_bake %>%
  ggplot(aes(___, ___, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

```{r umap-par-solution}
set.seed(123) # set a seed to reproduce random number generation
library(embed) # load the library to use `step_umap()`  

## Create the recipe accordingly
umap_rec <- recipe(~., data = zoo) %>%
  update_role(animal_name, animal_type, new_role = "id") %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_umap(all_predictors(), num_comp = 3)

## Prep your recipe
umap_prep <- prep(umap_rec)

## Extract UMAP components with bake
umap_bake <- bake(umap_prep, zoo)
umap_bake

## Plot UMAP components
umap_bake %>%
  ggplot(aes(umap_1, umap_3, label=animal_name)) +
  geom_point(aes(color = animal_type), alpha = 0.7, size = 2)+
  geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL) +
  theme_minimal()
```

Looks like increasing the number of UMAP components did not change our results that much. Try other hyperparameters and see if you can produce different plots. Don't forget to modify the plot accordingly.

